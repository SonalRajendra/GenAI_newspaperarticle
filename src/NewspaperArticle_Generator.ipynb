{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Environment secret keys\n",
        "from google.colab import userdata\n",
        "import os\n",
        "from langchain_community.retrievers import WikipediaRetriever\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "# Retrieve API token securely\n",
        "sec_key = userdata.get(\"HUGGINGFACEHUB_API_TOKEN\")\n",
        "\n",
        "# Ensure the API key exists\n",
        "if not sec_key:\n",
        "    raise ValueError(\"HUGGINGFACEHUB_API_TOKEN not found! Make sure it is set in Colab.\")\n",
        "\n",
        "# Set the environment variable\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = sec_key\n",
        "\n",
        "# Initialize Wikipedia Retriever\n",
        "wikipedia = WikipediaRetriever()\n",
        "\n",
        "# Initialize Hugging Face Model\n",
        "repo_id = \"deepseek-ai/DeepSeek-R1\"\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=repo_id,\n",
        "    task=\"text-generation\",\n",
        "    max_length=512,\n",
        "    min_length=250,\n",
        "    temperature=0.9\n",
        ")\n",
        "\n",
        "def generate_news_article(topic):\n",
        "    \"\"\"\n",
        "    Generates a well-structured news article using RAG (Wikipedia + LLM).\n",
        "    \"\"\"\n",
        "\n",
        "    # Retrieve relevant Wikipedia content\n",
        "    relevant_docs = wikipedia.get_relevant_documents(topic)\n",
        "    if not relevant_docs:\n",
        "        raise ValueError(f\"No Wikipedia content found for topic: {topic}\")\n",
        "\n",
        "    context = \"\\n\\n\".join([doc.page_content for doc in relevant_docs[:3]])  # Limit to 3 relevant docs\n",
        "\n",
        "    # Construct prompt with Wikipedia context\n",
        "    prompt = f\"\"\"\n",
        "    You are a professional journalist. Write a well-structured news article on '{topic}' in three paragraphs.\n",
        "\n",
        "    *Use the provided information from Wikipedia:*\n",
        "    {context}\n",
        "\n",
        "    *Requirements:*\n",
        "    1️⃣ Start with a **catchy and engaging headline**.\n",
        "    2️⃣ *First paragraph* → Introduce the topic with a compelling hook, latest news and  explain why it matters globally.\n",
        "    3️⃣ *Second paragraph* → Provide key details, background, and industry impact.\n",
        "    4️⃣ *Third paragraph* → Conclude with the broader implications and possible future developments.\n",
        "\n",
        "    Format the article in a continuous flow without subheadings or section markers.\n",
        "    \"\"\"\n",
        "\n",
        "    response = llm.invoke(prompt)\n",
        "    return response\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "u7gzMNhAQx9-",
        "outputId": "9fcfad13-6360-49bf-f28f-9bb56319cd3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_huggingface.llms.huggingface_endpoint:WARNING! max_length is not default parameter.\n",
            "                    max_length was transferred to model_kwargs.\n",
            "                    Please make sure that max_length is what you intended.\n",
            "WARNING:langchain_huggingface.llms.huggingface_endpoint:WARNING! min_length is not default parameter.\n",
            "                    min_length was transferred to model_kwargs.\n",
            "                    Please make sure that min_length is what you intended.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to format paragraphs with new line after every 15 tokens and 2 line spaces between paragraphs\n",
        "def format_paragraphs(news_article):\n",
        "    paragraphs = news_article.strip().split('\\n')\n",
        "    formatted_output = \"\"\n",
        "\n",
        "    for paragraph in paragraphs:\n",
        "        words = paragraph.split()  # Tokenize into words\n",
        "        formatted_paragraph = \"\"\n",
        "\n",
        "        # Add new line after every 15 words\n",
        "        for i in range(0, len(words), 15):\n",
        "            formatted_paragraph += \" \".join(words[i:i+15]) + \"\\n\"\n",
        "\n",
        "        # Add two line spaces between paragraphs\n",
        "        formatted_output += formatted_paragraph + \"\\n\"\n",
        "\n",
        "    return formatted_output\n"
      ],
      "metadata": {
        "id": "lt1D4oDxQzON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Get formatted output\n",
        "topic = \"Ola Kaellenius\"\n",
        "news_article = generate_news_article(topic)\n",
        "formatted_article = format_paragraphs(news_article)\n",
        "\n",
        "# Print the result\n",
        "print(\"\\nGenerated Newspaper Article:\\n\")\n",
        "print(formatted_article)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7n27N80pSHE1",
        "outputId": "551ebb1b-2bd5-4f1b-d4ab-5153d7b22c34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated Newspaper Article:\n",
            "\n",
            "Examples of newspapers: [The Guardian](https://www.theguardian.com/international), [BBC News](https://www.bbc.com/news/world), [The New York Times](https://www.nytimes.com/international/)\n",
            "\n",
            "\n",
            "\n",
            "Headline:\n",
            "\n",
            "Ola Kaellenius: The Steward of Mercedes-Benz's Alabama Plant and Its Evolution\n",
            "\n",
            "\n",
            "The man behind the wheel of Mercedes-Benz's largest SUV plant in the world, Ola Kaellenius,\n",
            "has been instrumental in the Alabama facility's growth and diversification. Kaellenius, a Swede by birth,\n",
            "took the helm of Mercedes-Benz U.S. International (MBUSI) in 2009, succeeding Bill Taylor. His tenure\n",
            "coincided with a significant shift in the plant's production portfolio, including the introduction of the\n",
            "C-Class and the move towards SUVs and electric vehicles.\n",
            "\n",
            "\n",
            "Under Kaellenius's leadership, MBUSI expanded its offerings to include not only the popular GLE, GLE\n",
            "Coupe, and GLS models but also the high-end Mercedes-Maybach GLS. More recently, the plant started\n",
            "production of the EQE SUV and EQS SUV, marking Mercedes-Benz's entry into the electric SUV\n",
            "market. Kaellenius's vision and strategic planning have turned the Vance, Alabama facility into a global\n",
            "powerhouse for Mercedes-Benz, exporting vehicles to over 135 countries worldwide.\n",
            "\n",
            "\n",
            "Kaellenius's impact on MBUSI extended beyond the production line. He oversaw the construction of the\n",
            "Mercedes-Benz Visitor Center, which opened in 2013, offering plant tours and housing a museum showcasing\n",
            "the brand's rich history. Moreover, Kaellenius navigated the plant through labor relations challenges, including a\n",
            "recent UAW unionization attempt in 2024. Despite these hurdles, MBUSI continues to thrive as a\n",
            "significant economic contributor to the state of Alabama, employing over 4,000 people. Kaellenius's influence on\n",
            "the plant's success is undeniable, leaving a lasting legacy that will continue to shape Mercedes-Benz's\n",
            "global production strategy.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!jupyter nbconvert --to html Article_Generator.ipynb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1u24EeyxnFd",
        "outputId": "f93a67e8-d980-4382-ec55-bcef9972bd44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NbConvertApp] Converting notebook Article_Generator.ipynb to html\n",
            "[NbConvertApp] Writing 330681 bytes to Article_Generator.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EKdx1B-byK-X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}